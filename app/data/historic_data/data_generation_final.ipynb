{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _objective_ of this notebook is to generate synthetic data which has the same variability as the historical data. In particular, we want the synthetic data to have the same covariance matrix as the historical data.\n",
    "## Which variables should be included in this covariance matrix?\n",
    "Some of the variables do not look Normal, and therefore the multivariate_normal method will always fail to make data that resembles the original distributions. Based on the histograms, Treasury Rate, home price index, mortgage rate, rent index, and vacancy rate seem more like uniform distributions (though home price index looks potentially like a bimodal Normal). I would exclude these variables from this data generation process. In the last cell, I generated new data without these variables included in the covariance matrix. The plots didn't change much, but at least it's easier to focus on the variables where you have a chance at modelling them as Normal.\n",
    "### End goal -> Input needed\n",
    "The output of this data generation should be return rates that can be multiplied by a portfolio value to give a return. Due to the time-dependency of some of the indexes, indexes are not always as valuable a output as the derivative of the indexes. We need the values that are going to show the most accurate/useful co-variances.\n",
    "#### | Value | Index | Derivative | Which to use |\n",
    "\n",
    "\n",
    "| Equity | S&P 500 Index | Return rate | Derivative\n",
    "\n",
    "| Inflation | CPI | Inflation rate | CPI\n",
    "\n",
    "| Bonds | Treasury 10-Year Yield | Rate of change | Index\n",
    "\n",
    "| RE Appreciation | Home Price Index | Appreciation | Derivative\n",
    "\n",
    "| RE Borrowing Cost | 30-Year Fixed Rate Mortgage | Rate of change | Index\n",
    "\n",
    "| RE Income | Rent Index | Rate of change | Derivative\n",
    "\n",
    "| RE Vacancy | Rental Vacancy Rate | Rate of change | Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np,pandas as pd\n",
    "import matplotlib.pyplot as plt, seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP500_quart_rate = pd.read_csv('SP500_quart_rate.csv',header=0,names=['DATE','SP500_Return'])\n",
    "inflation_quart_rate = pd.read_csv('inflation_quart_rate.csv',header=0,names=['DATE','Inflation_Rate'])\n",
    "treasury_bond_quart_rate = pd.read_csv('treasury_bond_quart_rate.csv',header=0,names=['DATE','Treasury_Yield','Treasury_Yield_Change'])\n",
    "home_price_quart_rate = pd.read_csv('home_price_quart_rate.csv',header=0,names=['DATE','Home_Price_Index','Home_Price_Appreciation'])\n",
    "mortgage_quart_rate = pd.read_csv('mortgage_quart_rate.csv',header=0,names=['DATE','Mortgage_Rate','Mortgage_Rate_Change'])\n",
    "rent_quart_rate = pd.read_csv('rent_quart_rate.csv',header=0,names=['DATE','Rent_Index','Rent_Change'])\n",
    "vacancy_quart_rate = pd.read_csv('vacancy_quart_rate.csv',header=0,names=['DATE','Vacancy_Rate','Vacancy_Rate_Change'])\n",
    "\n",
    "merged_df = SP500_quart_rate.merge(inflation_quart_rate, how='inner',left_on='DATE', right_on='DATE')\n",
    "merged_df = merged_df.merge(treasury_bond_quart_rate, how='left',left_on='DATE', right_on='DATE')\n",
    "merged_df = merged_df.merge(home_price_quart_rate, how='left',left_on='DATE', right_on='DATE')\n",
    "merged_df = merged_df.merge(mortgage_quart_rate, how='left',left_on='DATE', right_on='DATE')\n",
    "merged_df = merged_df.merge(rent_quart_rate, how='left',left_on='DATE', right_on='DATE')\n",
    "merged_df = merged_df.merge(vacancy_quart_rate, how='left',left_on='DATE', right_on='DATE')\n",
    "merged_df.drop(columns=['Treasury_Yield_Change','Home_Price_Index','Mortgage_Rate_Change','Rent_Index','Vacancy_Rate_Change'],inplace=True)\n",
    "merged_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['DATE']= pd.to_datetime(merged_df['DATE'])\n",
    "sn.set_style('whitegrid')\n",
    "sn.relplot(data=merged_df, x='DATE', y='SP500_Return',\n",
    "           aspect=3\n",
    "          )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.relplot(data=merged_df, x='DATE', y='Inflation_Rate',\n",
    "           aspect=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax= plt.subplots(1,2, figsize=(10,4), dpi=150)\n",
    "sn.scatterplot(data=merged_df, x='SP500_Return',\n",
    "            y='Inflation_Rate', ax=ax[0])\n",
    "ax[0].set_title(f\"from {merged_df['DATE'].min().date()} to {merged_df['DATE'].max().date()}\")\n",
    "start_date='1985-01-01'\n",
    "sn.scatterplot(data=merged_df[merged_df['DATE']>=start_date], \n",
    "               x='SP500_Return', y='Inflation_Rate', ax=ax[1])\n",
    "ax[1].set_title(f\"from {start_date} to {merged_df['DATE'].max().date()}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covMatrix = merged_df.cov()\n",
    "print(covMatrix)\n",
    "# sn.heatmap(covMatrix, vmin=-1, vmax=1, center=0)\n",
    "sn.heatmap(covMatrix, center=0)\n",
    "plt.title('Change columns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we visualize the r values of correlations, which are scale invariant, unlike the covariances (and variances). From this heatmap, it's clear to see that S&P500 return is not correlated with any of the variables, execpt for home price appreciation (but only slightly). On the other hand, inflation rate is positively correlated with Treasury yield, home price appreciation, mortgage rate, and rent change. Inflation rate is also negatively correlated with vacancy rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig= plt.figure(dpi=100)\n",
    "sn.heatmap(merged_df.corr(), center=0, vmin=-1, vmax=1, annot=True)\n",
    "plt.title(f\"from {merged_df['DATE'].min().date()} to {merged_df['DATE'].max().date()}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap below shows the correlations for the same variables but only looking at the data after 1985. Most of the relationships change. In particular, the correlation between S&P return and inflation rate becomes quite clear. In fact, this is the strongest positive correlation between inflation rate and any other variable.\n",
    "As I suspected, these correlations depend on the time period you're looking at. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shorter time frame for looking at correlation\n",
    "fig= plt.figure(dpi=100)\n",
    "sn.heatmap(merged_df[merged_df['DATE']>=start_date].corr(), \n",
    "           center=0, vmin=-1, vmax=1, annot=True)\n",
    "plt.title(f\"from {start_date} to {merged_df['DATE'].max().date()}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the heatmap below, you can see the covariance matrix after the individual variables are standardized. Notice that it is the same as the heatmap from corr()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#input data must be centered and scaled to unit variance\n",
    "scaler= StandardScaler()\n",
    "X= scaler.fit_transform(merged_df.drop(columns='DATE'))\n",
    "#convert back to frame for convenience\n",
    "X_df= pd.DataFrame(X, columns=merged_df.drop(columns='DATE').columns)\n",
    "covMatrix = X_df.cov()\n",
    "sn.heatmap(covMatrix , center=0)\n",
    "plt.title(f\"from {merged_df['DATE'].min().date()} to {merged_df['DATE'].max().date()}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covMatrix = X_df.cov().values\n",
    "mu = X_df.mean().values\n",
    "print(\"Shape of covariance matrix\",covMatrix.shape)\n",
    "print(\"Shape of averages\", mu.shape)\n",
    "num_samples = 300\n",
    "rng = np.random.default_rng()\n",
    "gen_data = rng.multivariate_normal(mu, covMatrix, size=num_samples)\n",
    "#change the scale of the generated data back to the original scale of the variables\n",
    "gen_data = scaler.inverse_transform(gen_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: need to eliminate at least 1 variable from covariance matrix \n",
    "The warning from the previous cell is very important. It tells us that the covariance matrix is not positive-semidefinite. The method won't give us useful results until we fix our covariance matrix. If calculate the eigenvalues (below) we find that 1 of them is negative, which means we just need to eliminate 1 of our variables to get a positive-semidefinite matrix. Essentially, one of the variables is far too similar to the other ones and we need to eliminate it. \n",
    "\n",
    "After some trial and error, I found that either treasury yield or mortgage rate could be eliminated to make the covariance matrix positive-semidefinite. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg\n",
    "eig_val, _ = linalg.eig(covMatrix)\n",
    "print(eig_val)\n",
    "if any(eig_val<0):\n",
    "    print('The covariance matrix is not positive-semidefinite')\n",
    "else:\n",
    "    print('The covariance matrix is positive-semidefinite')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col2use=merged_df.drop(columns=['DATE', 'Mortgage_Rate']).columns\n",
    "#create frame with standardized columns\n",
    "X_df= pd.DataFrame(scaler.fit_transform(merged_df[col2use]), \n",
    "                   columns=col2use)\n",
    "covMatrix = X_df.cov()\n",
    "eig_val, _= linalg.eig(covMatrix)\n",
    "print(eig_val)\n",
    "if any(eig_val<0):\n",
    "    print('The covariance matrix is not positive-semidefinite')\n",
    "else:\n",
    "    print('The covariance matrix is positive-semidefinite')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col2use=merged_df.drop(columns=['DATE', 'Mortgage_Rate',\n",
    "                               ]).columns\n",
    "#create frame with standardized columns\n",
    "scaler.fit(merged_df[col2use])\n",
    "X_df= pd.DataFrame(scaler.transform(merged_df[col2use]), \n",
    "                   columns=col2use)\n",
    "covMatrix = X_df.cov()\n",
    "eig_val, _= linalg.eig(covMatrix)\n",
    "print(eig_val)\n",
    "if any(eig_val<0):\n",
    "    print('The covariance matrix is not positive-semidefinite')\n",
    "else:\n",
    "    print('The covariance matrix is positive-semidefinite')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covMatrix = X_df.cov().values\n",
    "mu = X_df.mean().values\n",
    "print(\"Shape of covariance matrix\",covMatrix.shape)\n",
    "print(\"Shape of averages\", mu.shape)\n",
    "num_samples = 300\n",
    "rng = np.random.default_rng()\n",
    "gen_data = rng.multivariate_normal(mu, covMatrix, size=num_samples)\n",
    "#change the scale of the generated data back to the original scale of the variables\n",
    "generated_df= pd.DataFrame(scaler.inverse_transform(gen_data), \n",
    "                           columns=col2use)\n",
    "generated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(10,4), dpi=100, sharey=True)\n",
    "sn.heatmap(merged_df[col2use].corr(), center=0, annot=True, ax=ax[0])\n",
    "ax[0].set_title('original')\n",
    "sn.heatmap(generated_df.corr(), center=0, annot=True, ax=ax[1])\n",
    "ax[1].set_title('generated')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I correctly assigned the variable names to the columns of generated data by creating the dataframe generated_df. Not only did this dataframe allow us to conveniently calculate the heatmaps above, this dataframe also allows us to correctly see the histograms below. _Previosuly_, you left gen_data as an array and assumed that your for loop would correctly match the columns of gen_data to merged_df. But this assumption was flawed and that's why the plots made it look like the original data was vastly different from the generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in col2use:\n",
    "    plt.figure(dpi=80)\n",
    "    plt.title(col)\n",
    "    #use the same bins for both plots and normalize the density\n",
    "    _, bins, _= plt.hist(merged_df[col], bins=20, label='original', density=True)\n",
    "    plt.hist(generated_df[col],label='generated', bins=bins, density=True, alpha=0.7)\n",
    "    plt.ylabel('probability density')\n",
    "    plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
